{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85c10a86",
   "metadata": {},
   "source": [
    "# Aggregations\n",
    "\n",
    "**Aggregation** = Summarizing data by applying a function over groups or entire datasets\n",
    "\n",
    "- Usually **group by one or more keys (columns)** and apply an **aggregation function** (like sum, count, avg).\n",
    "- Used for **reporting, dashboards, statistics, KPIs**\n",
    "\n",
    "### 6 Types of Aggregations in Spark\n",
    "\n",
    "**1. Simple Aggregation:** Applies aggregation to the *entire DataFrame.*\n",
    "    \n",
    "    - Total items sold and average price across all transactions.\n",
    "\n",
    "**2. groupBy:** Group rows by one or more columns and apply aggregations.\n",
    "    \n",
    "    - Show how many items were sold by country.\n",
    "\n",
    "**3. Window Aggregation:** Similar to groupBY, but operates *relative to the current row* (eg. moving averages, rankings)\n",
    "    \n",
    "    - Rank sales records within each country.\n",
    "\n",
    "**4. Grouping Sets:** Combine different groupings in one query (e.g., group by Country, and Product, and both)\n",
    "    \n",
    "    - Combine multiple level of aggregation in one query (e.g. total by country and total by product separately.\n",
    "\n",
    "**5. Rollup (Heirarchial Summary:** Computes *sub-total levels* moving top-down in hierarchy.\n",
    "    \n",
    "    - Show subtotals by country, then a grand total at the bottom.\n",
    "\n",
    "**6. Cube (All Combinations:** Like rollup, but also computes *cross-tabulated summaries*.\n",
    "    \n",
    "    - Show totals by country, totals by product, and grand total - all in one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d000ee8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/18 19:42:13 WARN Utils: Your hostname, Satkars-MacBook-Pro.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.101 instead (on interface en0)\n",
      "25/07/18 19:42:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/18 19:42:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.101:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Aggregations</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x169b95850>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Aggregations\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a289b1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# lets begin by reading in our data on purchases + repartition + cache for rapid access:\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/Users/satkarkarki/spark_the_definitive_guide/data/retail-data/all/online-retail-dataset.csv\") \\\n",
    "    .coalesce(5)\n",
    "\n",
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c61c077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bd574ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM dfTable LIMIT 5\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40f4616e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c3f0a1",
   "metadata": {},
   "source": [
    "## Aggregation Functions\n",
    "\n",
    "- All aggregations are available as functions.\n",
    "- Most aggregation functions can be called from the **pyspark.sql.functions** in PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b976bcd8",
   "metadata": {},
   "source": [
    "##### Count is actually an action opposed to a transformation, so it returns immediately.\n",
    "    - Use count to get an idea of the total size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9672bfa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ## Applying the most basic aggregation i.e. count to the entire DataFrame:\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a101961c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() == 541909"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f6419",
   "metadata": {},
   "source": [
    "#### Using count() as a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0807cb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df.select(count(\"StockCode\")).show()\n",
    "\n",
    "## In SQL:\n",
    "## SELECT COUNT(*) FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9743bf8",
   "metadata": {},
   "source": [
    "##### countDisctinct() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d4595a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(\"StockCode\")).show()\n",
    "\n",
    "## In SQL:\n",
    "## SELECT COUNT(DISTINCT *) FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5426f276",
   "metadata": {},
   "source": [
    "##### approx_count_distinct\n",
    "    - useful when working with large datasets\n",
    "    - approximates the count till a certain degree of accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b52f1a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show()\n",
    "\n",
    "## IN SQL:\n",
    "## SELECT approx_count_distinct(StockCode, 0.1 FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef85414",
   "metadata": {},
   "source": [
    "##### first and last\n",
    " - retrieves first and last values (obvious from the name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ff5b4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "|first(StockCode)|last(StockCode)|\n",
      "+----------------+---------------+\n",
      "|          85123A|          22138|\n",
      "+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()\n",
    "\n",
    "## In SQL:\n",
    "## SELECT first(StockCode), last(StockCode) FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46016784",
   "metadata": {},
   "source": [
    "##### min and max\n",
    "  - Extract the minimum and maximum values from a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "390b2fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show(2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368ba848",
   "metadata": {},
   "source": [
    "##### sum\n",
    "    - Another simple task is to add all the values in a row using the sum function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7502f741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(\"Quantity\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "221f494d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satkarkarki/anaconda3/lib/python3.11/site-packages/pyspark/sql/functions/builtin.py:1780: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n",
      "  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "## Another variant is the sumDistinct function \n",
    "from pyspark.sql.functions import sumDistinct\n",
    "df.select(sumDistinct(\"Quantity\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddeb9f5",
   "metadata": {},
   "source": [
    "##### avg\n",
    "- the average function (sum/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5c3f055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------+----------------+\n",
      "|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "|                      9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+--------------------------------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, expr\n",
    "\n",
    "df.select(\n",
    "        count(\"Quantity\").alias(\"total_transactions\"),\n",
    "        sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "        avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "        expr(\"mean(Quantity)\").alias(\"mean_purchases\")) \\\n",
    "    .selectExpr(\n",
    "        \"total_purchases/total_transactions\",\n",
    "        \"avg_purchases\",\n",
    "        \"mean_purchases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578b066b",
   "metadata": {},
   "source": [
    "##### Variance and Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d287dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+---------------------+\n",
      "| var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "|47559.303646609274| 47559.39140929897|  218.08095663447847|   218.08115785023466|\n",
      "+------------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop, var_samp, stddev_samp\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),\n",
    "         stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()\n",
    "\n",
    "# -- in SQL\n",
    "# SELECT var_pop(Quantity), var_samp(Quantity),\n",
    "# stddev_pop(Quantity), stddev_samp(Quantity)\n",
    "# FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3659ad6",
   "metadata": {},
   "source": [
    "##### Skewness and Kurtosis\n",
    "\n",
    "- lets calculate the measureemnts of extreme points the head and the tail around the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb1fe73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "| skewness(Quantity)|kurtosis(Quantity)|\n",
      "+-------------------+------------------+\n",
      "|-0.2640755761052443|119768.05495533928|\n",
      "+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in Python\n",
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()\n",
    "\n",
    "\n",
    "# -- in SQL\n",
    "# SELECT skewness(Quantity), kurtosis(Quantity) FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7920ef23",
   "metadata": {},
   "source": [
    "##### covariance and correlation:\n",
    "\n",
    "- functions used to compare the interactions of two values in two differenct columns together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77854df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+------------------------------+-----------------------------+\n",
      "|corr(Quantity, Quantity)|covar_samp(Quantity, Quantity)|covar_pop(Quantity, Quantity)|\n",
      "+------------------------+------------------------------+-----------------------------+\n",
      "|                     1.0|             47559.39140929898|            47559.30364660928|\n",
      "+------------------------+------------------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "df.select(corr(\"Quantity\", \"Quantity\"), covar_samp(\"Quantity\", \"Quantity\"), \n",
    "          covar_pop(\"Quantity\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4c8b77",
   "metadata": {},
   "source": [
    "##### Aggregating to Complex Types\n",
    "\n",
    "- Summarizes a column by collecting all unique values (set) and all values incl. duplicates (list) into arrays.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75e1de57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Netherlands, Sau...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()\n",
    "\n",
    "## In SQL\n",
    "## SELECT collect_set(Country), collect_set(Country) FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fb7524",
   "metadata": {},
   "source": [
    "### Grouping\n",
    "\n",
    "- A more common task is to perform calculations based on **groups** in the data.\n",
    "- Grouping is typically done on categorical data.\n",
    "- Let's kick things off with some simple grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef198399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "|   538800|     16458|   10|\n",
      "|   538942|     17346|   12|\n",
      "|  C539947|     13854|    1|\n",
      "|   540096|     13253|   16|\n",
      "|   540530|     14755|   27|\n",
      "|   541225|     14099|   19|\n",
      "|   541978|     13551|    4|\n",
      "|   542093|     17677|   16|\n",
      "|   543188|     12567|   63|\n",
      "|   543590|     17377|   19|\n",
      "|  C543757|     13115|    1|\n",
      "|  C544318|     12989|    1|\n",
      "|   544578|     12365|    1|\n",
      "|   545165|     16339|   20|\n",
      "|   545289|     14732|   30|\n",
      "+---------+----------+-----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f2f99e",
   "metadata": {},
   "source": [
    "###### Grouping with Expressions\n",
    "\n",
    "- Group data and apply aggregations using both function-style and SQL-expression style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25476309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------------+\n",
      "|InvoiceNo|quan|count(Quantity)|\n",
      "+---------+----+---------------+\n",
      "|   536596|   6|              6|\n",
      "|   536938|  14|             14|\n",
      "|   537252|   1|              1|\n",
      "|   537691|  20|             20|\n",
      "|   538041|   1|              1|\n",
      "|   538184|  26|             26|\n",
      "|   538517|  53|             53|\n",
      "|   538879|  19|             19|\n",
      "|   539275|   6|              6|\n",
      "|   539630|  12|             12|\n",
      "|   540499|  24|             24|\n",
      "|   540540|  22|             22|\n",
      "|  C540850|   1|              1|\n",
      "|   540976|  48|             48|\n",
      "|   541432|   4|              4|\n",
      "|   541518| 101|            101|\n",
      "|   541783|  35|             35|\n",
      "|   542026|   9|              9|\n",
      "|   542375|   6|              6|\n",
      "|  C542604|   8|              8|\n",
      "+---------+----+---------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "    count(\"Quantity\").alias(\"quan\"),\n",
    "    expr(\"count(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935ba788",
   "metadata": {},
   "source": [
    "#### Grouping with Maps\n",
    "\n",
    "- Groups data and applies multiple SQL-style aggregations in-line using expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d9385b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   536596|               1.5|  1.1180339887498947|\n",
      "|   536938|33.142857142857146|  20.698023172885524|\n",
      "|   537252|              31.0|                 0.0|\n",
      "|   537691|              8.15|   5.597097462078001|\n",
      "|   538041|              30.0|                 0.0|\n",
      "|   538184|12.076923076923077|   8.142590198943392|\n",
      "|   538517|3.0377358490566038|  2.3946659604837897|\n",
      "|   538879|21.157894736842106|  11.811070444356483|\n",
      "|   539275|              26.0|  12.806248474865697|\n",
      "|   539630|20.333333333333332|  10.225241100118645|\n",
      "|   540499|              3.75|  2.6653642652865788|\n",
      "|   540540|2.1363636363636362|  1.0572457590557278|\n",
      "|  C540850|              -1.0|                 0.0|\n",
      "|   540976|10.520833333333334|   6.496760677872902|\n",
      "|   541432|             12.25|  10.825317547305483|\n",
      "|   541518| 23.10891089108911|  20.550782784878713|\n",
      "|   541783|11.314285714285715|   8.467657556242811|\n",
      "|   542026| 7.666666666666667|   4.853406592853679|\n",
      "|   542375|               8.0|  3.4641016151377544|\n",
      "|  C542604|              -8.0|  15.173990905493518|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\"))\\\n",
    "    .show()\n",
    "\n",
    "## IN SQL:\n",
    "## SELECT avg(Quantity), stddev_pop(Quantity), InvoiceNo FROM dfTable\n",
    "## GROUP BY InvoiceNo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab26a06",
   "metadata": {},
   "source": [
    "## Window Functions\n",
    "\n",
    "- A `GROUP BY` aggregates data so that each group becomes a single row.\n",
    "\n",
    "**Example:** Grouping by `ClientID` gives you one row per client, such as their total lifetime spend.\n",
    "\n",
    "- A **Window Function** lets you compute a value for every row, while still referencing a group of rows (called a *window* or *frame*).\n",
    "\n",
    "**Example:** For each transaction row, you can compute that client’s total spend over 5 years or their running total to date, without collapsing the rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28b30da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's convert the invoice date to contain only date info (not time info)\n",
    "\n",
    "from pyspark.sql.functions import col, to_date\n",
    "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"M/d/yyyy H:mm\"))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bee096a",
   "metadata": {},
   "source": [
    "###### The first step to a window function is to create a window specification\n",
    "- **Note:** `partitionBy` is unrelated to the partionining schem concept covered thus far\n",
    "- `rowsBetween` defines the **frame** for the window function, commonly used for **cumulative aggregations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f51f1367",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "windowSpec = Window \\\n",
    "    .partitionBy(\"CustomerId\", \"date\") \\\n",
    "    .orderBy(desc(\"Quantity\")) \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ff6cdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Applying an aggregation function to learn the maximum purchase quantity over all time:\n",
    "## This code will return a column which we can use in SELECT statement later:\n",
    "\n",
    "from pyspark.sql.functions import max\n",
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c34be758",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before proceeding to use the SELECT statement, we will create the purchase quantity rank (use dense_rank)\n",
    "## This code will return a column which we can use in SELECT statement later:\n",
    "\n",
    "from pyspark.sql.functions import dense_rank, rank\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "821dacf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|CustomerId|      date|Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|     12346|2011-01-18|   74215|           1|                1|              74215|\n",
      "|     12346|2011-01-18|  -74215|           2|                2|              74215|\n",
      "|     12347|2010-12-07|      36|           1|                1|                 36|\n",
      "|     12347|2010-12-07|      30|           2|                2|                 36|\n",
      "|     12347|2010-12-07|      24|           3|                3|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|       6|          17|                5|                 36|\n",
      "|     12347|2010-12-07|       6|          17|                5|                 36|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "## Using SELECT statement to view the calculated window values:\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\") \\\n",
    "    .select(\n",
    "        col(\"CustomerId\"),\n",
    "        col(\"date\"),\n",
    "        col(\"Quantity\"),\n",
    "        purchaseRank.alias(\"quantityRank\"),\n",
    "        purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "        maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a217d",
   "metadata": {},
   "source": [
    "## Grouping Sets\n",
    "\n",
    "- `Grouping Sets` lets you calculate **multiple** `GROUP BY` **aggregations** in a **single query**.\n",
    "- We'll have to use **Spark SQL** to perform grouping sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23d80188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple data cleaning aka null-cleansing:\n",
    "\n",
    "dfNoNull = dfWithDate.drop()\n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30c7a11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|CustomerId|stockCode|totalQuantity|\n",
      "+----------+---------+-------------+\n",
      "|     18287|    85173|           48|\n",
      "|     18287|   85040A|           48|\n",
      "|     18287|   85039B|          120|\n",
      "|     18287|   85039A|           96|\n",
      "|     18287|    84920|            4|\n",
      "|     18287|    84584|            6|\n",
      "|     18287|   84507C|            6|\n",
      "|     18287|   72351B|           24|\n",
      "|     18287|   72351A|           24|\n",
      "|     18287|   72349B|           60|\n",
      "|     18287|    47422|           24|\n",
      "|     18287|    47421|           48|\n",
      "|     18287|    35967|           36|\n",
      "|     18287|    23445|           20|\n",
      "|     18287|    23378|           24|\n",
      "|     18287|    23376|           48|\n",
      "|     18287|    23310|           36|\n",
      "|     18287|    23274|           12|\n",
      "|     18287|    23272|           12|\n",
      "|     18287|    23269|           36|\n",
      "+----------+---------+-------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CustomerId, \n",
    "        stockCode, \n",
    "        SUM(Quantity) AS totalQuantity\n",
    "    FROM dfNoNull\n",
    "    GROUP BY GROUPING SETS (\n",
    "        (CustomerId, stockCode),\n",
    "        (CustomerId),\n",
    "        (stockCode),\n",
    "        ()\n",
    "    )\n",
    "    ORDER BY CustomerId DESC NULLS LAST, stockCode DESC NULLS LAST\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a041c",
   "metadata": {},
   "source": [
    "#### ROLLUP\n",
    "\n",
    "- `ROLLUP` is a shortcut in Spark that automatically performs multiple levels of aggregation in **hierarchical order.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9906f6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+\n",
      "|      Date|       Country|total_quantity|\n",
      "+----------+--------------+--------------+\n",
      "|      NULL|          NULL|       5176450|\n",
      "|2010-12-01|United Kingdom|         23949|\n",
      "|2010-12-01|        France|           449|\n",
      "|2010-12-01|     Australia|           107|\n",
      "|2010-12-01|          NULL|         26814|\n",
      "|2010-12-01|        Norway|          1852|\n",
      "|2010-12-01|       Germany|           117|\n",
      "|2010-12-01|          EIRE|           243|\n",
      "|2010-12-01|   Netherlands|            97|\n",
      "|2010-12-02|          EIRE|             4|\n",
      "|2010-12-02|          NULL|         21023|\n",
      "|2010-12-02|United Kingdom|         20873|\n",
      "|2010-12-02|       Germany|           146|\n",
      "|2010-12-03|      Portugal|            65|\n",
      "|2010-12-03|        Poland|           140|\n",
      "|2010-12-03|       Belgium|           528|\n",
      "|2010-12-03|United Kingdom|         10439|\n",
      "|2010-12-03|        France|           239|\n",
      "|2010-12-03|         Italy|           164|\n",
      "|2010-12-03|          NULL|         14830|\n",
      "+----------+--------------+--------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF = dfNoNull.rollup(\"Date\", \"Country\").agg(sum(\"Quantity\")) \\\n",
    "    .selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as total_quantity\") \\\n",
    "    .orderBy(\"Date\")\n",
    "rolledUpDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf5752a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+\n",
      "|      Date|Country|total_quantity|\n",
      "+----------+-------+--------------+\n",
      "|      NULL|   NULL|       5176450|\n",
      "|2010-12-01|   NULL|         26814|\n",
      "|2010-12-02|   NULL|         21023|\n",
      "|2010-12-03|   NULL|         14830|\n",
      "|2010-12-05|   NULL|         16395|\n",
      "|2010-12-06|   NULL|         21419|\n",
      "|2010-12-07|   NULL|         24995|\n",
      "|2010-12-08|   NULL|         22741|\n",
      "|2010-12-09|   NULL|         18431|\n",
      "|2010-12-10|   NULL|         20297|\n",
      "|2010-12-12|   NULL|         10565|\n",
      "|2010-12-13|   NULL|         17623|\n",
      "|2010-12-14|   NULL|         20098|\n",
      "|2010-12-15|   NULL|         18229|\n",
      "|2010-12-16|   NULL|         29632|\n",
      "|2010-12-17|   NULL|         16069|\n",
      "|2010-12-19|   NULL|          3795|\n",
      "|2010-12-20|   NULL|         14965|\n",
      "|2010-12-21|   NULL|         15467|\n",
      "|2010-12-22|   NULL|          3192|\n",
      "+----------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "+----+-------+--------------+\n",
      "|Date|Country|total_quantity|\n",
      "+----+-------+--------------+\n",
      "|NULL|   NULL|       5176450|\n",
      "+----+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF.where(\"Country IS NULL\").show()\n",
    "rolledUpDF.where(\"Date IS NULL\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0165b836",
   "metadata": {},
   "source": [
    "##### Cube\n",
    "\n",
    "- A cube takes the rollup to a level deeper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2179b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "dfNoNull.cube(\"Date\", \"Country\").agg(sum(col(\"Quantity\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff1fa5",
   "metadata": {},
   "source": [
    "##### Grouping Metadata\n",
    "\n",
    "- Assigns a numeric code to each groups being aggregated so we know which row belongs to which level of aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88510b9e",
   "metadata": {},
   "source": [
    "## Pivot in Spark\n",
    "\n",
    "- Turn rows into columns and vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80c25e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = dfWithDate.groupBy(\"date\").pivot(\"Country\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a39efc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|      date|USA_sum(Quantity)|\n",
      "+----------+-----------------+\n",
      "|2011-12-06|             NULL|\n",
      "|2011-12-09|             NULL|\n",
      "|2011-12-08|             -196|\n",
      "|2011-12-07|             NULL|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted.where(\"date > '2011-12-05'\").select(\"date\" ,\"`USA_sum(Quantity)`\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921507e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c30840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
