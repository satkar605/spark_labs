{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f2bd26",
   "metadata": {},
   "source": [
    "# ðŸ’¡ Lab Introduction: Analyzing Retail Data with Structured Streaming in PySpark\n",
    "\n",
    "Welcome to this hands-on lab where we'll explore **Structured Streaming**, Spark's high-level API for real-time data processing. This lab bridges the gap between **batch processing** and **stream processing** by reusing the same DataFrame operations you've already seen, now applied to **streaming data**.\n",
    "\n",
    "## ðŸŽ¯ Objective\n",
    "\n",
    "We will simulate a real-time retail scenario where sales data is generated daily by multiple retail stores. By the end of this lab, you will:\n",
    "\n",
    "- Understand how to **read static data** using PySpark and define a schema.\n",
    "- Use **window functions** to group transactions over daily intervals.\n",
    "- Transition seamlessly from a **batch job to a streaming job** with minimal code changes.\n",
    "- Learn how Spark incrementally reads new data using `.readStream()` with `maxFilesPerTrigger`.\n",
    "- Output live streaming aggregations to:\n",
    "  - An **in-memory table** (for SQL queries)\n",
    "  - The **console** (for inspection during development)\n",
    "\n",
    "\n",
    "Each CSV file represents one day of transactions, making it ideal for **time-series analysis** and **stream simulation**.\n",
    "\n",
    "## ðŸ” Workflow Overview\n",
    "\n",
    "1. **Static Analysis**: Read all CSV files as a batch and explore using SQL + window functions.\n",
    "2. **Schema Extraction**: Capture the inferred schema for later use in streaming mode.\n",
    "3. **Streaming Setup**: Use `.readStream()` to incrementally process one file at a time.\n",
    "4. **Aggregation**: Compute total customer spending per day using time windows.\n",
    "5. **Action & Output**: Write the result to an in-memory table or console for live monitoring.\n",
    "\n",
    "---\n",
    "\n",
    "> âœ… This lab demonstrates how you can reuse familiar batch-based DataFrame logic in a streaming context, making real-time analytics more accessible and production-ready.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7f7621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85178c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StructuredStreamingLab\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14768680",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2501ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "staticDataFrame = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/Users/satkarkarki/spark_the_definitive_guide/data/retail-data/by-day/*.csv\")\n",
    "\n",
    "# staticDataFrame.createOrReplaceTempView(\"retail_data\")\n",
    "# staticSchema = staticDataFrame.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bbe583",
   "metadata": {},
   "outputs": [],
   "source": [
    "staticDataFrame.createOrReplaceTempView(\"retail_data\")\n",
    "staticSchema = staticDataFrame.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0aa27f",
   "metadata": {},
   "source": [
    "## Performing SQL query which calculates the sum of total costs per customer per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7d4b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, column, desc, col\n",
    "\n",
    "staticDataFrame\\\n",
    "    .selectExpr(\n",
    "        \"CustomerId\",\n",
    "        \"(UnitPrice * Quantity) as total_cost\",\n",
    "        \"InvoiceDate\")\\\n",
    "    .groupBy(\n",
    "        col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    "    .sum(\"total_cost\")\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68b81b9",
   "metadata": {},
   "source": [
    "**The configuration specifies the number of partitions that should be created afater a shuffle.**\n",
    "\n",
    "    - By default, the value is 200\n",
    "    - For local instances, it is better to set to 5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74507afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9876704",
   "metadata": {},
   "source": [
    "This block of code initiates a **streaming read operation**.\n",
    "\n",
    "readStream is used to **process data incrementally** like a live feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3637449",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingDataFrame = spark.readStream\\\n",
    "    .schema(staticSchema)\\\n",
    "    .option(\"maxFilesPerTrigger\", 1)\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(\"/Users/satkarkarki/spark_the_definitive_guide/data/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3df478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing a check to see whether our DataFrame is streaming\n",
    "\n",
    "streamingDataFrame.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad6f21",
   "metadata": {},
   "source": [
    "## The business logic set, this is still a lazy operation. We later need to call the streaming action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa23a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchaseByCustomerPerHour = streamingDataFrame\\\n",
    "    .selectExpr(\n",
    "            \"CustomerId\",\n",
    "            \"(UnitPrice * Quantity) AS total_cost\",\n",
    "            \"InvoiceDate\")\\\n",
    "        .groupBy(\n",
    "            col(\"CustomerID\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    "        .sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ddbf0",
   "metadata": {},
   "source": [
    "This block is the streaming action. Previously, we defined a streamingDataFrame, applied transformations.\n",
    "\n",
    "Since, transformations are lazy, we call the **streaming action** next:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e2cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "purchaseByCustomerPerHour.writeStream\\\n",
    "    .format(\"memory\")\\\n",
    "    .queryName(\"customer_purchases\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b522e",
   "metadata": {},
   "source": [
    "## Once we start the stream, we can run queries against it to debug what our result will look like if we were to write this out to the production sink:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0bc3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM customer_purchases\n",
    "    ORDER BY 'sum(total_cost)' DESC\n",
    "\"\"\")\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac0237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is still lazy â€” Spark plans, but doesn't act yet\n",
    "\n",
    "purchaseByCustomerPerHour = streamingDataFrame\\\n",
    "    .selectExpr(\"CustomerId\", \"(UnitPrice * Quantity) AS total_cost\", \"InvoiceDate\")\\\n",
    "    .groupBy(col(\"CustomerID\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n",
    "    .sum(\"total_cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bbce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the action â€” only now Spark starts executing\n",
    "\n",
    "purchaseByCustomerPerHour.writeStream\\\n",
    "    .format(\"memory\")\\\n",
    "    .queryName(\"customer_purchases\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4baa43e",
   "metadata": {},
   "source": [
    "## Machine Learning and Advanced Analyitcs\n",
    "\n",
    "    - Built-in library of machine learning called MLlib.\n",
    "    - MLlib allows for preprocessing, munging, training of models, and making predictions at scale on data.\n",
    "    \n",
    "For the purpose of this lab, we will perform some basic clustering on our data using k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729ccbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view schema for the DF:\n",
    "\n",
    "staticDataFrame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751be2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In MLlib, data must be represented as numerical values:\n",
    "# we'll use several DataFrame transformations to manipulate our date data:\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import date_format, col\n",
    "\n",
    "preppedDataFrame = staticDataFrame\\\n",
    "    .na.fill(0)\\\n",
    "    .withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\"))\\\n",
    "    .coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9112c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually creating train-test split:\n",
    "\n",
    "trainDataFrame = preppedDataFrame\\\n",
    "    .where(\"InvoiceDate < '2011-07-01'\")\n",
    "testDataFrame = preppedDataFrame\\\n",
    "    .where(\"InvoiceDate >= '2011-07-01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cd4f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataFrame.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bb9f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataFrame.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe37335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We perform now data transformations which will be covered extensively later (so just run):\n",
    "# This transformation will convert days of weeks into corresponding numerical values.\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer()\\\n",
    "    .setInputCol(\"day_of_week\")\\\n",
    "    .setOutputCol(\"day_of_week_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d625f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using one-hot encoding to create Boolean flags so each values get their own columns:\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "encoder = OneHotEncoder()\\\n",
    "    .setInputCol(\"day_of_week_index\")\\\n",
    "    .setOutputCol(\"day_of_week_encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241fe5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each of these will assemble in a set of columns which we need assemble into a vector next:\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler()\\\n",
    "    .setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\n",
    "    .setOutputCol(\"features\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc580b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we have three key features: the price, the quantity, and the day of week\n",
    "# Next, we'll set this up into a pipleline so that any future data we need to transform can go throiug hthe same process:\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transformationPipeline = Pipeline()\\\n",
    "    .setStages([indexer, encoder, vectorAssembler])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4d8eca",
   "metadata": {},
   "source": [
    "## Preparing for training is a two-step process:\n",
    "\n",
    "- We first need to fit our transformers to the dataset.\n",
    "- After that we are ready tottake that fitted pipeline and use it to transform al of our data in a consistent and repeatable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427c74ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedPipeline = transformationPipeline.fit(trainDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e930634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedTraining = fittedPipeline.transform(trainDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4e687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving on .cache() t okeep it in memory and save time for later use:\n",
    "\n",
    "transformedTraining.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfe5a83",
   "metadata": {},
   "source": [
    "## now its time to train the model, lets go:\n",
    "## in spark, training ML models happen in a two-phase process:\n",
    "\n",
    "    - Initialize an Untrained Model: **start an algorithm object**\n",
    "    - Fit the Model to Data **train it using .fit(data)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422eff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans()\\\n",
    "    .setK(20)\\\n",
    "    .setSeed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a5f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmModel = kmeans.fit(transformedTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a7409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training cost\n",
    "kmModel.summary.trainingCost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db94be09",
   "metadata": {},
   "source": [
    "## ðŸ”§ What are Lower-Level APIs?\n",
    "\n",
    "### In Spark, you have two major ways to work with data:\n",
    "\n",
    "- Level\tAPI\tDescription\n",
    "- High-Level\tDataFrame & Dataset APIs\tModern, optimized, SQL-like; easy to use\n",
    "- Low-Level\tRDDs (Resilient Distributed Datasets)\tManual control, lower-level operations\n",
    "\n",
    "The DataFrame API is built on top of RDDs â€” which means:\n",
    "\n",
    "Everything you do with DataFrames eventually becomes RDDs under the hood.\n",
    "\n",
    "But as a user, you donâ€™t need to touch RDDs 95% of the time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132212ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This turns the raw in-memory list into a distributed Spark DataFrame\n",
    "\n",
    "from pyspark.sql import Row\n",
    "spark.sparkContext.parallelize([Row(1), Row(2), Row(3)]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f10253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d80239c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4caf6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325fba22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
