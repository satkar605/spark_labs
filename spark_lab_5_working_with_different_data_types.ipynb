{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "027d5804",
   "metadata": {},
   "source": [
    "# Working with Different Types of Data\n",
    "\n",
    "In this lab, we look at working with a variety of different kinds of data in Spark including:\n",
    "\n",
    "    - Booleans\n",
    "    - Numbers\n",
    "    - Strings\n",
    "    - Dates and timestamps\n",
    "    - Handling null\n",
    "    - Complex types\n",
    "    - User-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f4c225df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.20.195.44:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DataTypesLab</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x16aea9410>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataTypesLab\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3d33d8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Loading the data required for this lab:\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/Users/satkarkarki/spark_the_definitive_guide/data/retail-data/by-day/2010-12-01.csv\")\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c7f41b",
   "metadata": {},
   "source": [
    "## Converting to Spark Types\n",
    "\n",
    "    - The lit function coverts a type in another language to its corresponding Spark representation.\n",
    "    - This concept will be used extensively throughout the lab.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b08a1dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[5: int, five: string, 5.0: double]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.select(lit(5), lit(\"five\"), lit(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6bdd04",
   "metadata": {},
   "source": [
    "## Working with Booleans\n",
    "\n",
    "    - Booleans are the foundation for all filtering.\n",
    "    - Four types of Booleans are: `and`, `or`, `true`, `false`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "662f84d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+\n",
      "|InvoiceNo|Description                  |\n",
      "+---------+-----------------------------+\n",
      "|536366   |HAND WARMER UNION JACK       |\n",
      "|536366   |HAND WARMER RED POLKA DOT    |\n",
      "|536367   |ASSORTED COLOUR BIRD ORNAMENT|\n",
      "|536367   |POPPY'S PLAYHOUSE BEDROOM    |\n",
      "|536367   |POPPY'S PLAYHOUSE KITCHEN    |\n",
      "+---------+-----------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# In Spark, it is a good practice to filter first then select because it optimizes our query\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "df.where(col(\"InvoiceNo\") != 536365) \\\n",
    "    .select(\"InvoiceNo\", \"Description\") \\\n",
    "    .show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c9dce2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                        |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER |6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n",
      "|536365   |71053    |WHITE METAL LANTERN                |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER     |8       |2010-12-01 08:26:00|2.75     |17850.0   |United Kingdom|\n",
      "|536365   |84029G   |KNITTED UNION FLAG HOT WATER BOTTLE|6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "|536365   |84029E   |RED WOOLLY HOTTIE WHITE HEART.     |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n",
      "+---------+---------+-----------------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# another cleaner option to pass the filter would be as follows:\n",
    "df.where(\"InvoiceNo = 536365\") \\\n",
    "    .show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "64e0f615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|Description                  |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n",
      "+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n",
      "|536366   |22633    |HAND WARMER UNION JACK       |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|\n",
      "|536366   |22632    |HAND WARMER RED POLKA DOT    |6       |2010-12-01 08:28:00|1.85     |17850.0   |United Kingdom|\n",
      "|536367   |84879    |ASSORTED COLOUR BIRD ORNAMENT|32      |2010-12-01 08:34:00|1.69     |13047.0   |United Kingdom|\n",
      "|536367   |22745    |POPPY'S PLAYHOUSE BEDROOM    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|\n",
      "|536367   |22748    |POPPY'S PLAYHOUSE KITCHEN    |6       |2010-12-01 08:34:00|2.1      |13047.0   |United Kingdom|\n",
      "+---------+---------+-----------------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# another cleaner option to pass the filter would be as follows:\n",
    "df.where(\"InvoiceNo <> 536365\") \\\n",
    "    .show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "55fdc414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      NULL|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      NULL|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use of chain filtering epxressions to pass multiple filters:\n",
    "from pyspark.sql.functions import instr\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(df.Description, \"POSTAGE\") >= 1\n",
    "df.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descripFilter).show()\n",
    "\n",
    "## In SQL:\n",
    "## SELECT * \n",
    "## FROM dfTable \n",
    "## WHERE StockCode in (\"DOT\") AND (UnitPrice > 600 OR instr(Description, \"POSTAGE\") >= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b20effc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|unitPrice|isExpensive|\n",
      "+---------+-----------+\n",
      "|   569.77|       true|\n",
      "|   607.49|       true|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## To filter a DataFrame, the following example shows how we can specify a Boolean column:\n",
    "\n",
    "from pyspark.sql.functions import instr\n",
    "\n",
    "DOTCodeFilter = col(\"StockCode\") == \"DOT\"\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1\n",
    "df.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter)) \\\n",
    "    .where(\"isExpensive\") \\\n",
    "    .select(\"unitPrice\", \"isExpensive\").show(5)\n",
    "\n",
    "## In SQL:\n",
    "## SELECT UnitPrice, (StockCOde = 'DOT' AND\n",
    "##    (UnitPrice > 600 OR instr(Description, \"POSTAGE\" >= 1)) AS isExpensive\n",
    "## FROM dfTable\n",
    "## WHERE (StockCode = 'DOT' AND\n",
    "##          (UnitPrice > 600 OR instr(Description, \"POSTAGE\") >= 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "93a6163d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|   Description|UnitPrice|\n",
      "+--------------+---------+\n",
      "|DOTCOM POSTAGE|   569.77|\n",
      "|DOTCOM POSTAGE|   607.49|\n",
      "+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 250\")) \\\n",
    "    .where(\"isExpensive\") \\\n",
    "    .select(\"Description\", \"UnitPrice\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb14563",
   "metadata": {},
   "source": [
    "## Working with Numbers\n",
    "\n",
    "- The second most common task after filtering in big data is counting things.\n",
    "- We do rounding\n",
    "- We compute correlation and then summary statistics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e190281f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "## Uses functional expressions, everything is Pythonic\n",
    "\n",
    "from pyspark.sql.functions import exp, pow\n",
    "\n",
    "fabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
    "df.select(expr(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ab4c7323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "## Uses a SQL expression string : one-off computation\n",
    "## fabricatedQuantity can be used later\n",
    "\n",
    "df.selectExpr(\n",
    "    \"CustomerId\",\n",
    "    \"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").show(2)\n",
    "\n",
    "## In SQL\n",
    "## SELECT CustomerID, (POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\n",
    "## FROM dfTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "260fa07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(7.5, 0)|bround(3.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          8.0|           4.0|\n",
      "|          8.0|           4.0|\n",
      "|          8.0|           4.0|\n",
      "|          8.0|           4.0|\n",
      "|          8.0|           4.0|\n",
      "+-------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "## Another common nummeric function is rounding:\n",
    "\n",
    "from pyspark.sql.functions import lit, round, bround\n",
    "\n",
    "df.select(round(lit(\"7.5\")), bround(lit(\"3.5\"))).show(5)\n",
    "\n",
    "## In SQL:\n",
    "## SELECT round(7.5), bround(3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ac5a73e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|corr(Quantity, UnitPrice)|\n",
      "+-------------------------+\n",
      "|     -0.04112314436835551|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Another common numeric function is correlation:\n",
    "\n",
    "from pyspark.sql.functions import corr\n",
    "df.stat.corr(\"Quantity\", \"UnitPrice\")\n",
    "df.select(corr(\"Quantity\", \"UnitPrice\")).show()\n",
    "\n",
    "## IN SQL:\n",
    "## SELECT corr(Quantity, UnitPrice) FROM dfTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "184d7d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|  count|             3108|              3108|                3098|              3108|              3108|              1968|          3108|\n",
      "|   mean| 536516.684944841|27834.304044117645|                NULL| 8.627413127413128| 4.151946589446603|15661.388719512195|          NULL|\n",
      "| stddev|72.89447869788873|17407.897548583845|                NULL|26.371821677029203|15.638659854603892|1854.4496996893627|          NULL|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|               0.0|           12431.0|     Australia|\n",
      "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|            607.49|           18229.0|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Another common task is computing the summary statistics:\n",
    "\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac92934d",
   "metadata": {},
   "source": [
    "### Working with Strings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5a20fa2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|initcap(Description)|\n",
      "+--------------------+\n",
      "|White Hanging Hea...|\n",
      "| White Metal Lantern|\n",
      "|Cream Cupid Heart...|\n",
      "|Knitted Union Fla...|\n",
      "|Red Woolly Hottie...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# the initcap function will capitalize every word in a given string when that word is separated from another space\n",
    "from pyspark.sql.functions import initcap\n",
    "df.select(initcap(col(\"Description\"))).show(5)\n",
    "\n",
    "## IN SQL:\n",
    "## SELECT initcap(Description) FROM dfTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "96f5061f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------------+\n",
      "|         Description|  lower(Description)|upper(lower(Description))|\n",
      "+--------------------+--------------------+-------------------------+\n",
      "|WHITE HANGING HEA...|white hanging hea...|     WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN| white metal lantern|      WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+-------------------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "# casting strings in uppercase and lowercase\n",
    "from pyspark.sql.functions import lower, upper\n",
    "df.select(col(\"Description\"),\n",
    "          lower(col(\"Description\")),\n",
    "          upper(lower(col(\"Description\")))).show(2)\n",
    "\n",
    "## IN SQL:\n",
    "## SELECT Description, lower(Description), Upper(lower(Description)) FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6597716",
   "metadata": {},
   "source": [
    "#### Another trivial task is adding ore removing spaces around a string.\n",
    "#### We can do this by using lpad, ltrim, rpad, rtrim, and trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e477bcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+---+----------+\n",
      "| ltrim| rtrim| trim| lp|        rp|\n",
      "+------+------+-----+---+----------+\n",
      "|HELLO | HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO | HELLO|HELLO|HEL|HELLO     |\n",
      "+------+------+-----+---+----------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
    "df.select(\n",
    "    ltrim(lit(\" HELLO \")).alias(\"ltrim\"),\n",
    "    rtrim(lit(\" HELLO \")).alias(\"rtrim\"),\n",
    "    trim(lit(\" HELLO \")).alias(\"trim\"),\n",
    "    lpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\n",
    "    rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")).show(2)\n",
    "\n",
    "# -- in SQL\n",
    "# SELECT\n",
    "#     ltrim(' HELLLOOOO '),\n",
    "#     rtrim(' HELLLOOOO '),\n",
    "#     trim(' HELLLOOOO '),\n",
    "#     lpad('HELLOOOO ', 3, ' '),\n",
    "#     rpad('HELLOOOO ', 10, ' ')\n",
    "# FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d69043c",
   "metadata": {},
   "source": [
    "#### Regular Expressions\n",
    "\n",
    "- In Spark, there are two key functions to perform regular expressions: regexp_extract and regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e23e32f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         color_clean|         Description|\n",
      "+--------------------+--------------------+\n",
      "|COLOR HANGING HEA...|WHITE HANGING HEA...|\n",
      "| COLOR METAL LANTERN| WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "# Replace color names with just \"COLOR\":\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "regex_string = \"BLACK|WHITE|RED|GREEN|BLUE\"\n",
    "df.select(\n",
    "    regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"),\n",
    "    col(\"Description\")).show(2)\n",
    "\n",
    "# -- in SQL\n",
    "# SELECT\n",
    "#    regexp_replace(Description, 'BLACK|WHITE|RED|GREEN|BLUE', 'COLOR') as\n",
    "#    color_clean, Description\n",
    "# FROM dfTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e13ab785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+--------------------+\n",
      "|translate(Description, LEET, 1337)|         Description|\n",
      "+----------------------------------+--------------------+\n",
      "|              WHI73 HANGING H3A...|WHITE HANGING HEA...|\n",
      "|               WHI73 M37A1 1AN73RN| WHITE METAL LANTERN|\n",
      "+----------------------------------+--------------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "# Spark provides a translation function to replace a string value :\n",
    "\n",
    "from pyspark.sql.functions import translate\n",
    "df.select(translate(col(\"Description\"), \"LEET\", \"1337\" ), col(\"Description\")) \\\n",
    "    .show(2)\n",
    "\n",
    "## IN SQL:\n",
    "## SELECT translate(Description, 'LEET', '1337')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1724acb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "## We can use the instr function to check of the existence of certain string values rather than extract them:\n",
    "from pyspark.sql.functions import instr\n",
    "containsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\n",
    "containsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\n",
    "df.withColumn(\"hasSimpleColor\", containsBlack | containsWhite) \\\n",
    "    .where(\"hasSimpleColor\") \\\n",
    "    .select(\"Description\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1039ba9",
   "metadata": {},
   "source": [
    "### Working with Dates and Timestamps\n",
    "\n",
    "**Spark** handles two time types:\n",
    "\n",
    "- DateType --> only stores calendar date (e.g. 2025-07-14)\n",
    "- TimestampType --> stores both date and exact time (e.g. 2025-07-14 13:22:00)\n",
    "\n",
    "**Spark** tires to auto-detect dates using **inferSchema=True** (this works well **if the format is standard**\n",
    "\n",
    "**Print schema often** using *df.printSchema()* to stay on track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2d070486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This code creates a DataFrame with 10 rows and a single column id values from 0 to 9.\n",
    "# Adds two columns with the current calendar row, and the exact current timestamp, respectively.\n",
    "# Stores int a temp table & prints the schema next:\n",
    "\n",
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "dateDF = spark.range(10) \\\n",
    "    .withColumn(\"today\", current_date()) \\\n",
    "    .withColumn(\"now\", current_timestamp()) \n",
    "\n",
    "dateDF.createOrReplaceTempView(\"dataTable\")\n",
    "\n",
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a518fe2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+\n",
      "| id|     today|                 now|\n",
      "+---+----------+--------------------+\n",
      "|  0|2025-07-15|2025-07-15 18:17:...|\n",
      "|  1|2025-07-15|2025-07-15 18:17:...|\n",
      "|  2|2025-07-15|2025-07-15 18:17:...|\n",
      "|  3|2025-07-15|2025-07-15 18:17:...|\n",
      "|  4|2025-07-15|2025-07-15 18:17:...|\n",
      "|  5|2025-07-15|2025-07-15 18:17:...|\n",
      "|  6|2025-07-15|2025-07-15 18:17:...|\n",
      "+---+----------+--------------------+\n",
      "only showing top 7 rows\n"
     ]
    }
   ],
   "source": [
    "dateDF.show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb75855c",
   "metadata": {},
   "source": [
    "#### Now lets add and subtract five days from today:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "39d8f307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2025-07-10|        2025-07-20|\n",
      "+------------------+------------------+\n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub\n",
    "dateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b06d0",
   "metadata": {},
   "source": [
    "#### Now lets calculate the difference between the number of days, we can do this datediff() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3be5f1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date Difference in Days\n",
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "Date Difference in Months\n",
      "+--------------------------------+\n",
      "|months_between(start, end, true)|\n",
      "+--------------------------------+\n",
      "|                    -16.67741935|\n",
      "+--------------------------------+\n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, to_date\n",
    "\n",
    "print(\"Date Difference in Days\")\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7)) \\\n",
    "    .select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)\n",
    "\n",
    "print(\"Date Difference in Months\")\n",
    "dateDF.select(\n",
    "    to_date(lit(\"2016-01-01\")).alias(\"start\"),\n",
    "    to_date(lit(\"2017-05-22\")).alias(\"end\")) \\\n",
    "    .select(months_between(col(\"start\"), col(\"end\"))).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a6945a",
   "metadata": {},
   "source": [
    "### Working with Nulls in Data\n",
    "- In Spark, null is the **preferred way to represent missing or empty data** in a DataFrame.\n",
    "- Spark **optimizes better for** null **values** than for placeholders like empty strings \"\", -999, or \"NA\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61749ce",
   "metadata": {},
   "source": [
    "###### Coalesce\n",
    "\n",
    "- returns the **first non-null value** from the list of columns **for each row**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "97d0f0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            fallback|\n",
      "+--------------------+\n",
      "|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN|\n",
      "|CREAM CUPID HEART...|\n",
      "|KNITTED UNION FLA...|\n",
      "|RED WOOLLY HOTTIE...|\n",
      "|SET 7 BABUSHKA NE...|\n",
      "|GLASS STAR FROSTE...|\n",
      "|HAND WARMER UNION...|\n",
      "|HAND WARMER RED P...|\n",
      "|ASSORTED COLOUR B...|\n",
      "|POPPY'S PLAYHOUSE...|\n",
      "|POPPY'S PLAYHOUSE...|\n",
      "|FELTCRAFT PRINCES...|\n",
      "|IVORY KNITTED MUG...|\n",
      "|BOX OF 6 ASSORTED...|\n",
      "|BOX OF VINTAGE JI...|\n",
      "|BOX OF VINTAGE AL...|\n",
      "|HOME BUILDING BLO...|\n",
      "|LOVE BUILDING BLO...|\n",
      "|RECIPE BOX WITH M...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, coalesce\n",
    "\n",
    "df.select(\n",
    "    coalesce(col(\"Description\").cast(\"string\"), col(\"CustomerId\").cast(\"string\")).alias(\"fallback\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24c4c6b",
   "metadata": {},
   "source": [
    "##### ifnull, nullif, nvl, and nvl2 are some SQL functions that can be used\n",
    "##### another sweet cornerstone about dealing with null values is to drop rows with null values using na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c9c2f991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drops all null values:\n",
    "df.na.drop(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7d5c5167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also apply that to certain sets of columns:\n",
    "df.na.drop(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c1798",
   "metadata": {},
   "source": [
    "##### The fill function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "079f62e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.fill(\"All Null values become this string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9b059d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We can also do this with Scala style mapping:\n",
    "fill_cols_vals = {\"StockCode\": 5, \"Description\": \"No Value\"}\n",
    "df.na.fill(fill_cols_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb29c96",
   "metadata": {},
   "source": [
    "##### replace() is another cool alternative to using drop and fill to handle null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "22300905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.replace([\"\"],[\"UNKNOWN\"],\"Description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf37df5",
   "metadata": {},
   "source": [
    "## Working with Complex Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e337f26c",
   "metadata": {},
   "source": [
    "### Structs\n",
    "- Structs are DataFrames within DataFrames\n",
    "- It allows multiple columns together into a single nested field. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fec88546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[complex: struct<Description:string,InvoiceNo:string>, InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.selectExpr(\"(Description, InvoiceNo) as complex\", \"*\")\n",
    "\n",
    "df.selectExpr(\"struct(Description, InvoiceNo) as complex\", \"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8dd99944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.createOrReplaceTempView(\"complexDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "895e0f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             complex|\n",
      "+--------------------+\n",
      "|{WHITE HANGING HE...|\n",
      "|{WHITE METAL LANT...|\n",
      "|{CREAM CUPID HEAR...|\n",
      "|{KNITTED UNION FL...|\n",
      "|{RED WOOLLY HOTTI...|\n",
      "|{SET 7 BABUSHKA N...|\n",
      "|{GLASS STAR FROST...|\n",
      "|{HAND WARMER UNIO...|\n",
      "|{HAND WARMER RED ...|\n",
      "|{ASSORTED COLOUR ...|\n",
      "|{POPPY'S PLAYHOUS...|\n",
      "|{POPPY'S PLAYHOUS...|\n",
      "|{FELTCRAFT PRINCE...|\n",
      "|{IVORY KNITTED MU...|\n",
      "|{BOX OF 6 ASSORTE...|\n",
      "|{BOX OF VINTAGE J...|\n",
      "|{BOX OF VINTAGE A...|\n",
      "|{HOME BUILDING BL...|\n",
      "|{LOVE BUILDING BL...|\n",
      "|{RECIPE BOX WITH ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "complexDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "037cad41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[complex.Description: string]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complexDF.select(\"complex.Description\")\n",
    "complexDF.select(col(\"complex\").getField(\"Description\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fdd2d368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Description: string, InvoiceNo: string]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complexDF.select(\"complex.*\")\n",
    "# -- in SQL\n",
    "# SELECT complex.* FROM complexDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7cebfa",
   "metadata": {},
   "source": [
    "## Arrays\n",
    "\n",
    "- In Spark, an **array** is a **complex column type** that can hold multiple values inside one cell.\n",
    "- Can think of like a list in Python.\n",
    "\n",
    "Lab Objective: Take each sentence (product description) in the Description column and **split it into words**, turning it into an **arrays of strings**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7d6760a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|split(Description,  , -1)|\n",
      "+-------------------------+\n",
      "|     [WHITE, HANGING, ...|\n",
      "|     [WHITE, METAL, LA...|\n",
      "+-------------------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "# split is used to break a string into an array using a delimiter:\n",
    "from pyspark.sql.functions import split \n",
    "\n",
    "df.select(split(col(\"Description\"), \" \")).show(2)\n",
    "\n",
    "## In SQL:\n",
    "## SELECT split(Description, ' ') FROM dfTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "733647ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|array_col[0]|\n",
      "+------------+\n",
      "|       WHITE|\n",
      "|       WHITE|\n",
      "+------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "## We can also query the values of the array using Python-like syntax which makes it more powerful:\n",
    "## we are only selecting the first string off the array:\n",
    "df.select(split(col(\"Description\"), \" \").alias(\"array_col\")) \\\n",
    "    .selectExpr(\"array_col[0]\").show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3035a2",
   "metadata": {},
   "source": [
    "### Array Length:\n",
    "\n",
    "We can determine the array’s length by querying for its size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4c2ae117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|size(split(Description,  , -1))|\n",
      "+-------------------------------+\n",
      "|                              5|\n",
      "|                              3|\n",
      "+-------------------------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import size\n",
    "df.select(size(split(col(\"Description\"), \" \"))).show(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "022793da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+\n",
      "|array_contains(split(Description,  , -1), WHITE)|\n",
      "+------------------------------------------------+\n",
      "|                                            true|\n",
      "|                                            true|\n",
      "+------------------------------------------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "## We can also see whether this array contains a value:\n",
    "\n",
    "from pyspark.sql.functions import array_contains\n",
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)\n",
    "\n",
    "## IN SQL:\n",
    "## SELECT array_contains(split(Description, ' '), 'WHITE') FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9e35b5",
   "metadata": {},
   "source": [
    "#### explode() --> takes an array column and turns each element int oa new row\n",
    "\n",
    "- Useful in tokenizing text into words\n",
    "- Breaking down multi-valued columns (easier to filter/count)\n",
    "- Flattening nested structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2a188f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------+\n",
      "|         Description|InvoiceNo|exploded|\n",
      "+--------------------+---------+--------+\n",
      "|WHITE HANGING HEA...|   536365|   WHITE|\n",
      "|WHITE HANGING HEA...|   536365| HANGING|\n",
      "|WHITE HANGING HEA...|   536365|   HEART|\n",
      "|WHITE HANGING HEA...|   536365| T-LIGHT|\n",
      "|WHITE HANGING HEA...|   536365|  HOLDER|\n",
      "| WHITE METAL LANTERN|   536365|   WHITE|\n",
      "| WHITE METAL LANTERN|   536365|   METAL|\n",
      "| WHITE METAL LANTERN|   536365| LANTERN|\n",
      "|CREAM CUPID HEART...|   536365|   CREAM|\n",
      "|CREAM CUPID HEART...|   536365|   CUPID|\n",
      "+--------------------+---------+--------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "df.withColumn(\"splitted\", split(col(\"Description\"), \" \")) \\\n",
    "    .withColumn(\"exploded\", explode(col(\"splitted\"))) \\\n",
    "    .select(\"Description\", \"InvoiceNo\", \"exploded\").show(10)\n",
    "\n",
    "## IN SQL:\n",
    "## SELECT Description, InvoiceNo, exploded\n",
    "## FROM (SELECT *, split(Description, \" \") as splitted FROM dfTable)\n",
    "## LATERAL VIEW explode(splitted) as exploded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974289fb",
   "metadata": {},
   "source": [
    "#### Map() in Spark:\n",
    "\n",
    "- A **map** is a **collection of key-value pairs**, like a Python dictionary.\n",
    "- In Spark, we can use maps to store **column values as keys and values**, and query them later.\n",
    "- Store related fields in one column and enable fast lookup by key.\n",
    "- Explode them for flat transformations per use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0a582627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+\n",
      "|complex_map                                    |\n",
      "+-----------------------------------------------+\n",
      "|{WHITE HANGING HEART T-LIGHT HOLDER -> 536365} |\n",
      "|{WHITE METAL LANTERN -> 536365}                |\n",
      "|{CREAM CUPID HEARTS COAT HANGER -> 536365}     |\n",
      "|{KNITTED UNION FLAG HOT WATER BOTTLE -> 536365}|\n",
      "|{RED WOOLLY HOTTIE WHITE HEART. -> 536365}     |\n",
      "+-----------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Coupling Description and Invoice No. into a map\n",
    "\n",
    "from pyspark.sql.functions import create_map\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")) \\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a361501e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|complex_map[WHITE METAL LANTERN]|\n",
      "+--------------------------------+\n",
      "|                            NULL|\n",
      "|                          536365|\n",
      "+--------------------------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "## Querying a map using proper key. A missing key returns null:\n",
    "\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")) \\\n",
    "    .selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ce48ee73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------+\n",
      "|key                                |value |\n",
      "+-----------------------------------+------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |536365|\n",
      "|WHITE METAL LANTERN                |536365|\n",
      "|CREAM CUPID HEARTS COAT HANGER     |536365|\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|536365|\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |536365|\n",
      "+-----------------------------------+------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "## We can also explode map types which will turn them into columns:\n",
    "\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")) \\\n",
    "    .selectExpr(\"explode(complex_map)\").show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66cb64e",
   "metadata": {},
   "source": [
    "## Working with JSON\n",
    "- **JSON**: Java Script Object Notation (presents data as **key-value pairs** and **ordered lists**.\n",
    "- Spark offers unique support for working with JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a6b7f4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's begin by creating a JSON column:\n",
    "\n",
    "jsonDF = spark.range(1).selectExpr(\"\"\"\n",
    "'{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "967dcdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|jsonString                                 |\n",
      "+-------------------------------------------+\n",
      "|{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}|\n",
      "+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f673a84",
   "metadata": {},
   "source": [
    "- We can use the *get_json_object* to inline query a JSON object, bet it a dictionary or a array.\n",
    "- We can use *json_tuple* if this objective has only one level of nesting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f6a444b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------+\n",
      "|column|c0                     |\n",
      "+------+-----------------------+\n",
      "|2     |{\"myJSONValue\":[1,2,3]}|\n",
      "+------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import get_json_object, json_tuple\n",
    "jsonDF.select(\n",
    "    get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\").alias(\"column\"),\n",
    "    json_tuple(col(\"jsonString\"), \"myJSONKey\")).show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0ecc1212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[to_json(MyStruct): string]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We can also turn a StructType into a JSON string by using the to_json function:\n",
    "\n",
    "from pyspark.sql.functions import to_json\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\") \\\n",
    "    .select(to_json(col(\"MyStruct\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a676efa9",
   "metadata": {},
   "source": [
    "##### This code convert two columns (InvoiceNo, Description) into a JSON string, and then parse that JSON back into a struct using a defined schema. This is often used when:\n",
    "\n",
    "- Storing structured data as JSON (eg. for APIs, logs ,or files)\n",
    "- Reading/parsing nested JSON from strings in a DataFrame\n",
    "- Testing or validating schema transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1ace525b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+-------------------------------------------------------------------------+\n",
      "|from_json(newJSON)                          |newJSON                                                                  |\n",
      "+--------------------------------------------+-------------------------------------------------------------------------+\n",
      "|{536365, WHITE HANGING HEART T-LIGHT HOLDER}|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\"}|\n",
      "|{536365, WHITE METAL LANTERN}               |{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE METAL LANTERN\"}               |\n",
      "+--------------------------------------------+-------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *\n",
    "parseSchema = StructType((\n",
    "    StructField(\"InvoiceNo\",StringType(),True),\n",
    "    StructField(\"Description\",StringType(),True)))\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\") \\\n",
    "    .select(to_json(col(\"myStruct\")).alias(\"newJSON\")) \\\n",
    "    .select(from_json(col(\"newJSON\"), parseSchema), col(\"newJSON\")).show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0ce331",
   "metadata": {},
   "source": [
    "## User-Defined Functions (UDF):\n",
    "\n",
    "- Spark allows you to define your own functions whihc makes it possible to write custom transformations.\n",
    "- Can write them in several programming languages (Python, Scala, Java, R)\n",
    "- By default, UDFs are temporty functions to be used in that specific SparkSession or Context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f8141e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udfExampleDF = spark.range(5).toDF(\"num\")\n",
    "def power3(double_value):\n",
    "    return double_value ** 3\n",
    "power3(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5136e74",
   "metadata": {},
   "source": [
    "##### Notes on UDFs:\n",
    "\n",
    "- UDFs are slower than native Spark functions because they break optimization.\n",
    "- Once a function is defined, Spark serializes the function on the driver node **the main program.**\n",
    "- If UDFs not written in JVM, it serializes all data from JVM into Pythonic format runs the function, and return back to the JVM format again. \n",
    "- **JVM** --> **Python** = **High Overhead**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "82d2ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, we register th function:\n",
    "from pyspark.sql.functions import udf\n",
    "power3udf = udf(power3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "849c06e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "+-----------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "## Then we can use it in our DataFrame code:\n",
    "from pyspark.sql.functions import col\n",
    "udfExampleDF.select(power3udf(col(\"num\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "687a119a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-07-15 18:18:44.260\", \"level\": \"ERROR\", \"logger\": \"SQLQueryContextLogger\", \"msg\": \"[UNRESOLVED_ROUTINE] Cannot resolve routine `power3` on search path [`system`.`builtin`, `system`.`session`, `spark_catalog`.`default`]. SQLSTATE: 42883\", \"context\": {\"errorClass\": \"UNRESOLVED_ROUTINE\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o1337.selectExpr.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_ROUTINE] Cannot resolve routine `power3` on search path [`system`.`builtin`, `system`.`session`, `spark_catalog`.`default`]. SQLSTATE: 42883; line 1 pos 0\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedRoutineError(QueryCompilationErrors.scala:907)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$18.applyOrElse(Analyzer.scala:2030)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$18.applyOrElse(Analyzer.scala:2011)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:475)\\n\\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1231)\\n\\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1230)\\n\\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:563)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:475)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:185)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:226)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:226)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:238)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:244)\\n\\tat scala.collection.immutable.List.map(List.scala:247)\\n\\tat scala.collection.immutable.List.map(List.scala:79)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:244)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:249)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:312)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:249)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:185)\\n\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:156)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:307)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:306)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:200)\\n\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:200)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:198)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:194)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:37)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:100)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:97)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:37)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning(AnalysisHelper.scala:306)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning$(AnalysisHelper.scala:303)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsDownWithPruning(LogicalPlan.scala:37)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning(AnalysisHelper.scala:278)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning$(AnalysisHelper.scala:276)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsWithPruning(LogicalPlan.scala:37)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2011)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2007)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\\n\\tat scala.collection.immutable.ArraySeq.foldLeft(ArraySeq.scala:222)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\\n\\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2263)\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:894)\\n\\tat org.apache.spark.sql.classic.Dataset.select(Dataset.scala:232)\\n\\tat org.apache.spark.sql.Dataset.selectExpr(Dataset.scala:1134)\\n\\tat org.apache.spark.sql.classic.Dataset.selectExpr(Dataset.scala:1900)\\n\\tat jdk.internal.reflect.GeneratedMethodAccessor81.invoke(Unknown Source)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedRoutineError(QueryCompilationErrors.scala:907)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$18.applyOrElse(Analyzer.scala:2030)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$$anonfun$apply$18.applyOrElse(Analyzer.scala:2011)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:475)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1231)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1230)\\n\\t\\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:563)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:475)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:185)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:226)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:226)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:238)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:244)\\n\\t\\tat scala.collection.immutable.List.map(List.scala:247)\\n\\t\\tat scala.collection.immutable.List.map(List.scala:79)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:249)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:312)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:249)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:185)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:156)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:307)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveExpressionsDownWithPruning$1.applyOrElse(AnalysisHelper.scala:306)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:200)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:200)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:198)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:194)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:37)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:100)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:97)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:37)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning(AnalysisHelper.scala:306)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsDownWithPruning$(AnalysisHelper.scala:303)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsDownWithPruning(LogicalPlan.scala:37)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning(AnalysisHelper.scala:278)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveExpressionsWithPruning$(AnalysisHelper.scala:276)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveExpressionsWithPruning(LogicalPlan.scala:37)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2011)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer$LookupFunctions$.apply(Analyzer.scala:2007)\\n\\t\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\\n\\t\\tat scala.collection.immutable.ArraySeq.foldLeft(ArraySeq.scala:222)\\n\\t\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\\n\\t\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\\n\\t\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 22 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/Users/satkarkarki/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/Users/satkarkarki/anaconda3/lib/python3.11/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_ROUTINE] Cannot resolve routine `power3` on search path [`system`.`builtin`, `system`.`session`, `spark_catalog`.`default`]. SQLSTATE: 42883; line 1 pos 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# in Python\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m udfExampleDF\u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpower3(num)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/sql/classic/dataframe.py:1005\u001b[0m, in \u001b[0;36mDataFrame.selectExpr\u001b[0;34m(self, *expr)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(expr) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(expr[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m   1004\u001b[0m     expr \u001b[38;5;241m=\u001b[39m expr[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m-> 1005\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jseq(expr))\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1363\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_ROUTINE] Cannot resolve routine `power3` on search path [`system`.`builtin`, `system`.`session`, `spark_catalog`.`default`]. SQLSTATE: 42883; line 1 pos 0"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef3bcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
