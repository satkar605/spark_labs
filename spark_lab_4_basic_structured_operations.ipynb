{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5114b67",
   "metadata": {},
   "source": [
    "# Basic Structured Operations\n",
    "\n",
    "- This lab is intended to focus on fundamental DataFrame operations.\n",
    "- We will use Spark to manipulate DataFrame operations and the data within them.\n",
    "- For the purpose of this lab, aggregations, window functions, and joins are spared.\n",
    "\n",
    "### Key Terminologies related to DataFrame\n",
    "\n",
    "- Consists of rows and columns.\n",
    "- Schemas define the name as well as the type of data in each column.\n",
    "- Partitioning defines the layout of DF and how it is distributed across the clusters.\n",
    "- The partitioning scheme defines how that is \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9988f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "703d9f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StructuredOpsLab\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39e1303b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.20.192.242:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>StructuredOpsLab</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x16377bf10>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5469788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a DataFrame to work with:\n",
    "df = spark.read.format(\"json\").load(\"/Users/satkarkarki/spark_the_definitive_guide/data/flight-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4d82dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets take a look at the schema of the current DataFrame:\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb663c5c",
   "metadata": {},
   "source": [
    "### Schema:\n",
    "    - Schema defines the column names and types of a DataFrame.\n",
    "    - We can either let a data source define the schema (called schema-on-read)\n",
    "        - or we can define it explicitly ourselves.\n",
    "    - A schema is a StructType made up of a number of fields, StructFields\n",
    "    - StructFields contain 'name', 'type', 'BooLean flag' (determine the null properties), and 'metadata' (optional)\n",
    "    - A schema in Spark can contain other comples StructTypes, to be featured in later chapters.\n",
    "    \n",
    "### The example that follows shows how to create and enforece a specific schema on a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7539f517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"json\").load(\"/Users/satkarkarki/spark_the_definitive_guide/data/flight-data/json/2015-summary.json\").schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac10c481",
   "metadata": {},
   "source": [
    "### This example that follows shows how to create and enforce a specific schema on DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20ef0744",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "    StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n",
    "])\n",
    "df=spark.read.format(\"json\").schema(myManualSchema)\\\n",
    "    .load(\"/Users/satkarkarki/spark_the_definitive_guide/data/flight-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534b643b",
   "metadata": {},
   "source": [
    "## Columns and Expressions\n",
    "\n",
    "    - Columns in Spark are similar to columns in a spreadsheet, R dataframe, or pandas DataFrame.\n",
    "    - The operations used to manipulate the columns from DataFrames are represented as expressions.\n",
    "    - The two simple functions used to construct and refer to columns are the <col> or <column> functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f02bc8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'someColumnName'>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, column\n",
    "col(\"someColumnName\")\n",
    "# column(\"someColumnName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf5066fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'<(-(*(+(someCol, 5), 200), 6), otherCol)'>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((col(\"someCol\") + 5) * 200) - 6) < col(\"otherCol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43370411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'(((someCol + 5) * 200) - 6) < otherCol'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "expr(\"(((someCol + 5) * 200) - 6) < otherCol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "227a342d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## accessing a DataFrame's columns:\n",
    "\n",
    "spark.read.format(\"json\")\\\n",
    "    .load(\"/Users/satkarkarki/spark_the_definitive_guide/data/flight-data/json/2015-summary.json\")\\\n",
    "    .columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a88939c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see a row by calling first on our DataFrame:\n",
    "\n",
    "\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "829405ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating rows:\n",
    "\n",
    "from pyspark.sql import Row\n",
    "myRow = Row(\"Hello\", None, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70ef0192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## to access data in rows, we just specify the postion that you would like:\n",
    "\n",
    "myRow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7cc5ce15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRow[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d9f57",
   "metadata": {},
   "source": [
    "## DataFrame Transformations:\n",
    "\n",
    "The most common DataFrame Trasnformations are:\n",
    "\n",
    "    - Remove columns or rows\n",
    "    - Transform a row into a column or column into row\n",
    "    - Add rows or columns\n",
    "    - Sort data by values in rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f828f",
   "metadata": {},
   "source": [
    "### Creating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fb933d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This query reads a json file into df which is later converted into a SQL table \"dfTable\":\n",
    "\n",
    "df = spark.read.format(\"json\").load(\"/Users/satkarkarki/spark_the_definitive_guide/data/flight-data/json/2015-summary.json\")\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d003607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# After registering the DataFrame as a Temp View: lets query some basic stuff from dfTable:\n",
    "spark.sql(\"SELECT * FROM dfTABLE LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "316c3c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| some| col|names|\n",
      "+-----+----+-----+\n",
      "|Hello|NULL|    1|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## The following block of codes walksthrough how DataFrame can be constructed on the fly:\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "myManualSchema = StructType([\n",
    "    StructField(\"some\", StringType(), True),\n",
    "    StructField(\"col\", StringType(), True),\n",
    "    StructField(\"names\", LongType(), False)    \n",
    "])\n",
    "myRow = Row(\"Hello\", None, 1)\n",
    "myDf = spark.createDataFrame([myRow], myManualSchema)\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841448fd",
   "metadata": {},
   "source": [
    "### select() and selectExpr()\n",
    "\n",
    "- **Select** and **SelectExpr** are the equivalent of SELECT (from SQL) in Spark with a slight different edge case\n",
    "- In simpler terms, it allows us to manipulate columns in the DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7175e34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DEST_COUNTRY_NAME\").show(2)\n",
    "\n",
    "## In SQL fashion, the same results can be queried through:\n",
    "\n",
    "## SELECT DEST_COUNTRY_NAME \n",
    "## FROM dfTable \n",
    "## LIMIT 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9f55ed9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "## Selecting multiple columns using select:\n",
    "\n",
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)\n",
    "\n",
    "# The SQL equivalent code for this would be:\n",
    "\n",
    "## SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME \n",
    "## FROM dfTable \n",
    "## LIMIT 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "02434c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|   DEST_COUNTRY_NAME|   DEST_COUNTRY_NAME|   DEST_COUNTRY_NAME|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|       United States|       United States|       United States|\n",
      "|       United States|       United States|       United States|\n",
      "|       United States|       United States|       United States|\n",
      "|               Egypt|               Egypt|               Egypt|\n",
      "|       United States|       United States|       United States|\n",
      "|       United States|       United States|       United States|\n",
      "|       United States|       United States|       United States|\n",
      "|          Costa Rica|          Costa Rica|          Costa Rica|\n",
      "|             Senegal|             Senegal|             Senegal|\n",
      "|             Moldova|             Moldova|             Moldova|\n",
      "|       United States|       United States|       United States|\n",
      "|       United States|       United States|       United States|\n",
      "|              Guyana|              Guyana|              Guyana|\n",
      "|               Malta|               Malta|               Malta|\n",
      "|            Anguilla|            Anguilla|            Anguilla|\n",
      "|             Bolivia|             Bolivia|             Bolivia|\n",
      "|       United States|       United States|       United States|\n",
      "|             Algeria|             Algeria|             Algeria|\n",
      "|Turks and Caicos ...|Turks and Caicos ...|Turks and Caicos ...|\n",
      "|       United States|       United States|       United States|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "## In Spark, columns can be refered in a number of different ways:\n",
    "## expr --> uses SQL-style string expression\n",
    "## col --> functional way to reference a column (most clean and common method in DataFrame transformations)\n",
    "## column --> Same as col, but often used in advanced dynamic logic (less common)\n",
    "\n",
    "from pyspark.sql.functions import expr, col, column\n",
    "df.select(\n",
    "    expr(\"DEST_COUNTRY_NAME\"),\n",
    "    col(\"DEST_COUNTRY_NAME\"),\n",
    "    column(\"DEST_COUNTRY_NAME\"))\\\n",
    "   .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6b77deab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "## Using expr to alias:\n",
    "\n",
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "43edfd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|Destination Country|\n",
      "+-------------------+\n",
      "|      United States|\n",
      "|      United States|\n",
      "+-------------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "## We can further manipulate the result of expression as another expression\n",
    "## This is where we strip off the aliasing done previously + another way to pass alias\n",
    "\n",
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\").alias(\"Destination Country\")) \\\n",
    "    .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cc1888c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "## We can use selectExpr as a simple way to build up complex expressions that create new DataFrames.\n",
    "## In this example, we create a new column (boolean) which checks if the flight is within country or not.\n",
    "\n",
    "df.selectExpr(\n",
    "    \"*\",\n",
    "    \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\") \\\n",
    "    .show(2)\n",
    "\n",
    "## The SQL equivalent for this code will be:\n",
    "## SELECT *, (DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) AS withinCountry\n",
    "## FROM dfTable\n",
    "## LIMIT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0b0a653f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n",
      "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+-----------+---------------------------------+\n",
      "|1770.765625|                              132|\n",
      "+-----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## With select expression, we can also specify aggregations over the entire DataFrame:\n",
    "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)\n",
    "\n",
    "## The SQL equivalent for this code will be:\n",
    "## SELECT AVG(count), COUNT(DISTINCT(DEST_COUNTRY_NAME))\n",
    "## FROM dfTable\n",
    "## LIMIT 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad08bf16",
   "metadata": {},
   "source": [
    "## Literals in Spark are used to pass on a constant value which needs to pass on as Spark Types\n",
    "\n",
    "#### This will come up when checking whether a value is greater than some constant or other programmatically created variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e8f98921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)\n",
    "\n",
    "## This is how literals are passed in SQL:\n",
    "## SELECT *, 1 AS One FROM dfTable LIMIT 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aeab7e",
   "metadata": {},
   "source": [
    "### Adding Columns\n",
    "\n",
    "- The formal way to add a new column to a DataFrame is using the withColumn() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee0cb768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"numberOne\", lit(1)).show(2)\n",
    "\n",
    "## SQL Equivalent Code:\n",
    "## SELECT *, 1 AS numberOne FROM dfTable LIMIT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1c9455d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "## In this example, we'll set a Boolean flag when the origin country = destination country\n",
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")) \\\n",
    "    .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148a93fd",
   "metadata": {},
   "source": [
    "#### Renaming Columns:\n",
    "- Use the withColumnRenamed method to rename existing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "27d1b50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dest', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f4c533",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c6b2d23",
   "metadata": {},
   "source": [
    "### Reserved Characters and Keywords\n",
    "\n",
    "- **Use backticks** when a column name has spaces, dashes, or is a SQL keyword **and** you are writing SQL-style expressions.\n",
    "- **Don't use backticks** when you're simply passing string names in Python methods like withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "076d26d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using withColumn() -- No escaping required:\n",
    "\n",
    "dfWithLongColName = df.withColumn(\n",
    "    \"This Long Column-Name\",\n",
    "    expr(\"ORIGIN_COUNTRY_NAME\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "06d067f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------+\n",
      "|This Long Column-Name|new col|\n",
      "+---------------------+-------+\n",
      "|              Romania|Romania|\n",
      "|              Croatia|Croatia|\n",
      "+---------------------+-------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "## Using selectExpr() -- Escaping Required:\n",
    "\n",
    "dfWithLongColName.selectExpr(\n",
    "    \"`This Long Column-Name`\",\n",
    "    \"`This Long Column-Name` AS `new col`\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9c09beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithLongColName.createOrReplaceTempView(\"dfTableLong\")\n",
    "dfWithLongColName.select(expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba171c3",
   "metadata": {},
   "source": [
    "### By default, Spark is case insensitive and it can be made case-sensitive by setting the configuration\n",
    "set spark.sql.caseSensitive true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5e72b7",
   "metadata": {},
   "source": [
    "### Changing a Column's Type (cast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "33832a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint, count2: string]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.withColumn(\"count2\", col(\"count\").cast(\"string\"))\n",
    "\n",
    "## In SQL:\n",
    "## SELECT *, CAST(count AS TEXT) AS count2 FROM dfTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06940bc5",
   "metadata": {},
   "source": [
    "### Filtering Rows\n",
    "\n",
    "    - There are two methods supporting filter operations: 'where' or 'filter'.\n",
    "    - While 'filter' is valid, its a common practice to stick with 'where'.\n",
    "    - Spark applies filter all at once, so be careful while applying multiple filters.\n",
    "    - Recommended to chain the filters in the right sequence to avoid applying them all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e284117f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "# Filter values where count is less than 2 (using 'filter'):\n",
    "\n",
    "df.filter(col(\"count\") < 2).show(2)\n",
    "\n",
    "## In SQL:\n",
    "## SELECT * FROM dfTable WHERE count < 2 LIMIT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8e639b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "# Filter values where count is less than 2 (using 'where'):\n",
    "\n",
    "df.where(\"count < 2\").show(2)\n",
    "\n",
    "## In SQL:\n",
    "## SELECT * FROM dfTable WHERE count < 2 LIMIT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7f54ebf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "# Applying multiple filters:\n",
    "\n",
    "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\") \\\n",
    "    .show(2)\n",
    "\n",
    "## In SQL:\n",
    "## SELECT * FROM dfTable WHERE count < 2 AND ORIGIN_COUNTRY_NAME != \"Croatia\" LIMIT 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07b0c99",
   "metadata": {},
   "source": [
    "### Getting Unique Rows\n",
    "\n",
    "        - Use the 'distinct' method to get unique rows; helps in the deduplication process.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ba998f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()\n",
    "\n",
    "## In SQL:\n",
    "## SELECT COUNT(DISTINCT(ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME)) FROM dfTable;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "333cf315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()\n",
    "\n",
    "## In SQL:\n",
    "## SELECT COUNT(DISTINCT(ORIGIN_COUNTRY_NAME)) FROM dfTable;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6155d793",
   "metadata": {},
   "source": [
    "## Random Samples\n",
    "\n",
    "    - Spark supports sampling random records uins the sample method on a DataFrame.\n",
    "    - withReplacement is a boolean expression that says whether samples can be replaced or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "36213549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 5\n",
    "withReplacement = False\n",
    "fraction = 0.5\n",
    "df.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3264f01",
   "metadata": {},
   "source": [
    "## Random Splits\n",
    "\n",
    "    - This feature is useful mostly in ML, to split dataset into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c99311e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrames = df.randomSplit([0.25, 0.75], seed)\n",
    "dataFrames[0].count() > dataFrames[1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638b69cb",
   "metadata": {},
   "source": [
    "### Concatenating and Appending Rows (Union)\n",
    "\n",
    "    - We can not directly add rows to an existing DataFrame like you might in pandas using .append()\n",
    "    - To append to a DataFrame, we must union the original DataFrame along with the new DataFrame.\n",
    "    - Use .union() to combine rows of 2 DataFrames but only if schemas match,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "71cd749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "schema = df.schema\n",
    "newRows = [\n",
    "    Row(\"New Country\", \"Other Country\", 5),\n",
    "    Row(\"New Country 2\", \"Other Country 3\", 1)\n",
    "]\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows) ## parallelize rows across Spark cluster\n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "53dc0341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|      New Country|      Other Country|    5|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.union(newDF) \\\n",
    "    .where(\"count = 5\") \\\n",
    "    .where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5862ef00",
   "metadata": {},
   "source": [
    "### Sorting Rows\n",
    "\n",
    "    - The two equivalent Spark operations of sorting rows are: sort and orderBy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "81234ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "|               Malta|      United States|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|            Suriname|      United States|    1|\n",
      "|       United States|             Cyprus|    1|\n",
      "|        Burkina Faso|      United States|    1|\n",
      "|            Djibouti|      United States|    1|\n",
      "|       United States|            Estonia|    1|\n",
      "|              Zambia|      United States|    1|\n",
      "|              Cyprus|      United States|    1|\n",
      "|       United States|          Lithuania|    1|\n",
      "|       United States|           Bulgaria|    1|\n",
      "|       United States|            Georgia|    1|\n",
      "|       United States|            Bahrain|    1|\n",
      "|       Cote d'Ivoire|      United States|    1|\n",
      "|       United States|   Papua New Guinea|    1|\n",
      "|              Kosovo|      United States|    1|\n",
      "|                Iraq|      United States|    1|\n",
      "|           Indonesia|      United States|    1|\n",
      "|       New Caledonia|      United States|    1|\n",
      "|       United States|         Montenegro|    1|\n",
      "|       United States|            Namibia|    1|\n",
      "|             Liberia|      United States|    2|\n",
      "|             Hungary|      United States|    2|\n",
      "|       United States|            Vietnam|    2|\n",
      "|            Malaysia|      United States|    2|\n",
      "|           Greenland|      United States|    2|\n",
      "|             Croatia|      United States|    2|\n",
      "|       United States|            Liberia|    2|\n",
      "|       United States|              Malta|    2|\n",
      "|             Georgia|      United States|    2|\n",
      "|               Niger|      United States|    2|\n",
      "|       United States|          Indonesia|    2|\n",
      "|       United States|           Malaysia|    3|\n",
      "|            Thailand|      United States|    3|\n",
      "|       United States|            Hungary|    3|\n",
      "|           Singapore|      United States|    3|\n",
      "|             Tunisia|      United States|    3|\n",
      "|    Papua New Guinea|      United States|    3|\n",
      "|            Bulgaria|      United States|    3|\n",
      "|       United States|           Thailand|    4|\n",
      "|             Algeria|      United States|    4|\n",
      "|       United States|          Greenland|    4|\n",
      "|       French Guiana|      United States|    5|\n",
      "|       United States|           Paraguay|    6|\n",
      "|            Pakistan|      United States|   12|\n",
      "|       United States|              Egypt|   12|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 50 rows\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"count\").show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "892a71b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|        Burkina Faso|      United States|    1|\n",
      "|       Cote d'Ivoire|      United States|    1|\n",
      "|              Cyprus|      United States|    1|\n",
      "|            Djibouti|      United States|    1|\n",
      "|           Indonesia|      United States|    1|\n",
      "|                Iraq|      United States|    1|\n",
      "|              Kosovo|      United States|    1|\n",
      "|               Malta|      United States|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "|       New Caledonia|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|            Suriname|      United States|    1|\n",
      "|       United States|            Estonia|    1|\n",
      "|       United States|             Cyprus|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|   Papua New Guinea|    1|\n",
      "|       United States|            Bahrain|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Lithuania|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e597edf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|        Burkina Faso|      United States|    1|\n",
      "|       Cote d'Ivoire|      United States|    1|\n",
      "|              Cyprus|      United States|    1|\n",
      "|            Djibouti|      United States|    1|\n",
      "|           Indonesia|      United States|    1|\n",
      "|                Iraq|      United States|    1|\n",
      "|              Kosovo|      United States|    1|\n",
      "|               Malta|      United States|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "|       New Caledonia|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|            Suriname|      United States|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|             Cyprus|    1|\n",
      "|       United States|            Estonia|    1|\n",
      "|       United States|          Lithuania|    1|\n",
      "|       United States|           Bulgaria|    1|\n",
      "|       United States|            Georgia|    1|\n",
      "|       United States|            Bahrain|    1|\n",
      "|       United States|   Papua New Guinea|    1|\n",
      "|       United States|         Montenegro|    1|\n",
      "|       United States|            Namibia|    1|\n",
      "|              Zambia|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 25 rows\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3aeacf",
   "metadata": {},
   "source": [
    "#### To more explicitly specify sort direction, we can also use the asc or desc functions if operation on a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e298eca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "df.orderBy(expr(\"count desc\").desc()).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c52272a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "## use asc_nulls_first, or desc_nulls_last to specify where we would like the nulls value to appear:\n",
    "\n",
    "df.orderBy(expr(\"count desc\").desc_nulls_first()).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "36e87431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorting partitions is also possible in Spark: (it is important for effective optimization)\n",
    "\n",
    "spark.read.format(\"json\").load(\"/Users/satkarkarki/spark_the_definitive_guide/data/flight-data/json/2015-summary.json\") \\\n",
    "    .sortWithinPartitions(\"count\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c231c9",
   "metadata": {},
   "source": [
    "### Repartition and Coalesce\n",
    "\n",
    "    - Another important optimization opportunity is to partition the data according to some frequently filtered columns:\n",
    "    - If we often run queries like WHERE country = 'USA', the partitioning in country can make those queries faster.\n",
    "    - That is because Spark can skip partitions taht don't match a filter - a concept known as partition pruning.\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "43df8cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[404] at coalesce at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c7a186f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# partitioning based on a certain column i.e. destination country name for this example:\n",
    "\n",
    "df.repartition(col(\"DEST_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c99b316e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optionally, the number of partitions to be applied can also be specified:\n",
    "\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77746966",
   "metadata": {},
   "source": [
    "### Coalesce\n",
    "\n",
    "- coalesce(n) reduces the number of partitions **without moving all the data around**.\n",
    "- It's a **narrow transformation** - only combines existing partitions together.\n",
    "- Spark **avoids reshuffling data across the cluster**, so it's **much cheaper than** repartition()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8ab9e9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff16b53a",
   "metadata": {},
   "source": [
    "## Collecting Rows to the Driver\n",
    "\n",
    "The **driver** is the **coordinator node** in Spark. It holds:\n",
    "\n",
    "    - The logic of the Spark app\n",
    "    - The result of transformatiuons and actions (when pulled from the cluster)\n",
    "    \n",
    "When we **collect data to the driver**, you are **moving data from the executors (cluster) to the local machine**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b089d12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "collectDF = df.limit(10) # applies transformation to limit the DataFrame to 10 rows\n",
    "collectDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9cab25bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF.take(5) # take works with an Integer count and returns the first 5 rows from collectDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f3b57fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "collectDF.show(5, False) ## false tells Spark not to turncate long strings in cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "68e40d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62),\n",
       " Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588),\n",
       " Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4126d24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _local_iterator_from_socket.<locals>.PyLocalIterable.__iter__ at 0x166099fc0>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## The method toLocalIterator collects partitions to the driver as an iterator\n",
    "collectDF.toLocalIterator()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
