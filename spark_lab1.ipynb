{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "494d1cad",
   "metadata": {},
   "source": [
    "## Starting Spark\n",
    "\n",
    "This notebook contains the practice codes from Chapter 1 and 2 of Spark: The Definitive Guide by Bill Chambers & Matei Zaharia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98ceb262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbc8c735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyFirstSparkNotebook\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a96b5490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.101:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyFirstSparkNotebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x127ca30d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c97a4ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates a DataFrame with one column containing 1,000 rows with values from 0 to 999\n",
    "myRange = spark.range(1000).toDF(\"number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f59317",
   "metadata": {},
   "source": [
    "## TRANSFORMATIONS\n",
    "Let's perform a simple trasnformation to find all event numbers in our current DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4119892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "divisby2 = myRange.where(\"number % 2 = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f6365",
   "metadata": {},
   "source": [
    "## ACTIONS\n",
    "To trigger the computation, we run an action.\n",
    "The simplest action is count, which gives us the total number fo records in the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66b4f555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divisby2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beba53f7",
   "metadata": {},
   "source": [
    "## An End-to-End Example:\n",
    "We’ll use Spark to analyze some flight data from the United States Bureau of\n",
    "Transportation statistics.\n",
    "\n",
    "Reading with Spark\n",
    "\n",
    "- spark.read() is used to access the DataFrameReader\n",
    "- .option(\"inferSchema\", \"true\") tells Spark to guess data types\n",
    "- .option(\"header\", \"true\") instructs taht first row contains column names\n",
    "- .csv() specifies format and path of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6620d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015 = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"data/flight-data/csv/2015-summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c4edbdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Executes the read + transformations\n",
    "# Pulls 3 rows from the distributed DataFrame\n",
    "# Converts it into a local Python list of Row objects\n",
    "flightData2015.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd5eadc",
   "metadata": {},
   "source": [
    "## Sorting\n",
    "This line does **not** execute anything. It only defines a transformation: \n",
    "    \n",
    "    - sort the DataFrame by the <count> column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d86cc0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#93 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(count#93 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=168]\n",
      "      +- FileScan csv [DEST_COUNTRY_NAME#91,ORIGIN_COUNTRY_NAME#92,count#93] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/satkarkarki/spark_the_definitive_guide/data/flight-data/cs..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d681d",
   "metadata": {},
   "source": [
    "## Configure Shuffle Output\n",
    "This sets the **number of output partitions** when a shuffle (like a sort) occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4ee9dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b3c35c",
   "metadata": {},
   "source": [
    "## Trigger the Job with .take()\n",
    "\n",
    "    - .take(2) is an action which:\n",
    "        - sorts the DataFrame\n",
    "        - Shuffles data\n",
    "        - Takes the first 2 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c6ad630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab18118",
   "metadata": {},
   "source": [
    "## What createOrReplaceTempView() Does\n",
    "This line **does not write anything to disk**. Instead, it:\n",
    "\n",
    "    - Registers the flightData2015 **DataFrame** as a **temporary SQL view** named \"flight_data_2015\"\n",
    "    - Makes the DataFrame **accessible via SQL syntax** in the same Spark session\n",
    "    - Allows us to **run SQL queries** on it just like we would on a SQL table.\n",
    "    \n",
    "Spark doesn’t distinguish between SQL and DataFrame code internally.\n",
    "Whether you use .filter(), .groupBy() in Python or SELECT ... WHERE ... in SQL —\n",
    "Spark compiles both into the same logical and physical plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4bb17fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a00978",
   "metadata": {},
   "source": [
    "- Now we can query our data in SQL.\n",
    "- To do so, we'll use the spark.sql function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db18adc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#91], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#91, 5), ENSURE_REQUIREMENTS, [plan_id=190]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#91], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#91] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/satkarkarki/spark_the_definitive_guide/data/flight-data/cs..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#91], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#91, 5), ENSURE_REQUIREMENTS, [plan_id=203]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#91], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#91] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/satkarkarki/spark_the_definitive_guide/data/flight-data/cs..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, count(1)\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")\n",
    "\n",
    "dataFrameWay = flightData2015\\\n",
    "    .groupby(\"DEST_COUNTRY_NAME\")\\\n",
    "    .count()\n",
    "\n",
    "sqlWay.explain()\n",
    "dataFrameWay.explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff5cfd",
   "metadata": {},
   "source": [
    "## Option 1: SQL Query Interface\n",
    "\n",
    "    We can write pure SQL using spark.sql() if we prefer a traditional, declarative style:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e10e3db3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT max(count) FROM flight_data_2015\").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a05320f",
   "metadata": {},
   "source": [
    "## Option 2: DataFrame API\n",
    "    We can use Spark's functional API to build transformations fluently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e63777c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "flightData2015.select(max(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad09411",
   "metadata": {},
   "source": [
    "Both methods complie to the **same logical plan** internally.\n",
    "That means:\n",
    "\n",
    "    - Same optimization engine\n",
    "    - Same performance\n",
    "    - Same execution DAG\n",
    "    \n",
    "**So it's really a matter of style and use case!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68530121",
   "metadata": {},
   "source": [
    "## A bit more advanced query: Find top five destination in the data\n",
    "This is our first multi-transformation query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f22624",
   "metadata": {},
   "source": [
    "## Option 1: SQL Query Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "98f6a1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxSql = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) AS destination_total\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21be0525",
   "metadata": {},
   "source": [
    "## Option 2: DatFrame Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6bcbcd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "flightData2015\\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    "    .sort(desc(\"destination_total\"))\\\n",
    "    .limit(5)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5b91f517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- TakeOrderedAndProject(limit=5, orderBy=[destination_total#181L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#91,destination_total#181L])\n",
      "   +- HashAggregate(keys=[DEST_COUNTRY_NAME#91], functions=[sum(count#93)])\n",
      "      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#91, 5), ENSURE_REQUIREMENTS, [plan_id=380]\n",
      "         +- HashAggregate(keys=[DEST_COUNTRY_NAME#91], functions=[partial_sum(count#93)])\n",
      "            +- FileScan csv [DEST_COUNTRY_NAME#91,count#93] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/satkarkarki/spark_the_definitive_guide/data/flight-data/cs..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015\\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    "    .sort(desc(\"destination_total\"))\\\n",
    "    .limit(5)\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87739d68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cff3fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c63aef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
